# -*- coding: utf-8 -*-
"""weather.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dARlKE1v2Uhd31aKo4A4zYzJAbHnHtkB

from google.colab import drive
drive.mount('/content/drive')

"""## 1. Descripción del dataset

Enlace del dataset:

http://www.bom.gov.au/climate/dwo/

http://www.bom.gov.au/climate/data/

Cada registro del dataset, además de las distintas mediciones metereológicas, contiene una columna binaria para indicar si ese día ha llovió y otra que indica si llovió al día siguiente. Por lo tanto, el objetivo de esta práctica es utilizar el dataset para generar un modelo de regresión que sea capaz de predecir si va a llover al día siguiente.

A continuación, una breve descripción de las variables de acuerdo con la información disponible en la página del departamento de metereología australiano http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml


"""

from prettytable import PrettyTable

data = [
    ["rowID", "ID del registro", "N/A"],
    ["Location", "Ubicación de la estación metereológica", "Nombre del lugar"],
    ["MinTemp", "Temperatura mínima", "Grados Celsius"],
    ["MaxTemp", "Temperatura máxima", "Grados Celsius"],
    ["Rainfall", "Precipitación", "Milímetros"],
    ["Evaporation", "Evaporación tipo A", "Milímetros"],
    ["Sunshine", "Horas de luz solar", "Horas"],
    ["WindGustDir", "Dirección de la ráfaga de viento más fuerte", "16 puntos cardinales"],
    ["WindGustSpeed", "Velocidad de la ráfaga de viento más fuerte", "Kilómetros por hora"],
    ["WindDir9am", "Dirección del viento a las 9am", "16 puntos cardinales"],
    ["WindDir3pm", "Dirección del viento a las 3pm", "16 puntos cardinales"],
    ["WindSpeed9am", "Velocidad del viento a las 9am", "Kilómetros por hora"],
    ["WindSpeed3pm", "Velocidad del viento a las 3pm", "Kilómetros por hora"],
    ["Humidity9am", "Humedad relativa a las 9am", "Porcentaje"],
    ["Humidity3pm", "Humedad relativa a las 3pm", "Porcentaje"],
    ["Pressure9am", "Presión atmosférica reducida al nivel medio del mar a las 9am", "Hectopascales"],
    ["Pressure3pm", "Presión atmosférica reducida al nivel medio del mar a las 3pm", "Hectopascales"],
    ["Cloud9am", "Fracción del cielo oscurecida por nubes a las 9am.", "Octavos"],
    ["Cloud3pm", "Fracción del cielo oscurecida por nubes a las 3pm", "Octavos"],
    ["Temp9am", "Temperatura a las 9am", "Grados Celsius"],
    ["Temp3pm", "Temperatura a las 3pm", "Grados Celsius"],
    ["RainToday", "¿Ha llovido en el día actual?", "Binario (sí o no)"],
    ["RainTomorrow", "¿Ha llovido al día siguiente?", "Binario (sí o no)"]
]

# Crear la tabla
table = PrettyTable()
table.field_names = ["Columna", "Definición", "Unidad"]
table.add_rows(data)

# Imprimir la tabla
print(table)

"""El dataset es importante porque ofrece una extensa colección de observaciones meteorológicas que son muy valiosas, ya que abarcan una variedad de datos meteorológicos útiles para realizar análisis y construir modelos predictivos.

La pregunta que se pretende abordar es la predicción de la lluvia. Con la presencia de las columnas binarias "RainToday" y "RainTomorrow", se busca entender y prever si lloverá al día siguiente. Este tipo de predicción puede ser crucial para diversas aplicaciones, como la planificación agrícola, la gestión del agua, la predicción de inundaciones y la toma de decisiones cotidianas basadas en las condiciones meteorológicas, como la planificación de eventos específicos, y ofrece información valiosa para diversas industrias y sectores que dependen de la información meteorológica para tomar decisiones informadas. Al desarrollar un modelo de regresión con este dataset, se busca entender los factores que influyen en la probabilidad de lluvia y construir un sistema que pueda hacer predicciones precisas.

A continuación, continuamos con un análisis general de las variables para conocer mejor el dataset y obtener más información.

En primer lugar importaremos las librerías necesarias para la práctica y cargaremos el dataset.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from tabulate import tabulate
from sklearn.impute import KNNImputer

weather2 = pd.read_csv('/content/drive/MyDrive/Weather.csv', sep=',')
weather = weather2

"""Mostramos las primeras filas para obtener una vista rápida de la estructura y el contenido."""

print(weather.head())

"""Nos interesa obtener información más detallada sobre la estructura del DataFrame:"""

print(weather.info())

"""Vemos que el DataFrame tiene 99516 filas y 23 columnas. La mayoría de las columnas son de tipo float64, estas son las observaciones con valores numéricos. Algunas, como WindDir9am y WindGustSpeed por ejemplo, son de tipo object ya que son colunmnas categóricas. "RainTomorrow" es de tipo int64, ya que está codificada con los valores 0 y 1.

A continuación echaremos un viztazo a las estadísticas descriptivas para las columnas numéricas, excluyendoo las columnas de tipo object, es decir, excluyendo las variables categóricas.
"""

print(weather.describe(exclude=[object]))

"""A continuación observamos las estadísticas descriptivas para las columnas categóricas."""

print(weather.describe(include=[object]))

"""Entre otras cosas, vemos que, por ejemplo, hay 49 ubicaciones únicas, siendo Canberra la más frecuente. Hay 16 direcciones únicas del viento, tal y como se indica en la descripción de las variables más arriba, y el oeste es la dirección de viento más frecuente cuando se trata de la ráfaga de viento más fuerte. Lo más común es que no llueva. Podemos visualizar mejor las distribuciones de las variables categóricas con gráficos que nos muestren el porcentaje de cada una."""

categorical_columns = ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']

# Función para generar histogramas de las categoricas con porcentajes
def percentage_bar_plot_with_labels(column):
    plt.figure(figsize=(12, 6))

    # Calculamos los porcentajes
    percentages = (weather[column].value_counts() / len(weather)) * 100

    # Gráfico de barras con porcentajes
    ax = sns.barplot(x=percentages.index, y=percentages.values)

    # Agregamos etiquetas con porcentajes en cada barra para visualizar mejor
    for p in ax.patches:
        ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                    textcoords='offset points')

    plt.title(f'Distribución de {column} (Porcentaje)')
    plt.xticks(rotation=45)
    plt.ylabel('Porcentaje')
    plt.show()

# Generamos los histogramas
for column in categorical_columns:
    percentage_bar_plot_with_labels(column)

"""Podemos observar que se registra lluvia el propio día en el 22.16% de los registros, frente a un 76.85% de no lluvia. Las proporciones de RainTomorrow son muy similares, como es de esperar.

A continuación realizaremos tareas de integración de datos y limpieza. Para ello creamos una copia del DataFrame original en el que guardaremos las modificaciones.
"""

# Creamos una copia del DataFrame original para hacer las modificaciones
weather_clean = weather.copy()

"""## Integración de datos

De cara a poder hacer predicciones mediantes regresiones más adelante, vamos a proceder a imputar los valores de las variables WindGustDir, WindDir9am, WindDir3pm y Location como medias de días de lluvia para cada valor de dicha variable.
"""

#Calculamos la media de dias de lluvia de las variables y sus valores y los imputamos a las propias variables

WindGustDir_mean = dict(round(weather_clean.groupby(['WindGustDir'])['RainTomorrow'].agg('sum') / weather_clean.groupby(['WindGustDir'])['RainTomorrow'].agg('count'),2))
WindDir9am_mean = dict(round(weather_clean.groupby(['WindDir9am'])['RainTomorrow'].agg('sum') / weather_clean.groupby(['WindDir9am'])['RainTomorrow'].agg('count'),2))
WindDir3pm_mean = dict(round(weather_clean.groupby(['WindDir3pm'])['RainTomorrow'].agg('sum') / weather_clean.groupby(['WindDir3pm'])['RainTomorrow'].agg('count'),2))
Location_mean = dict(round(weather_clean.groupby(['Location'])['RainTomorrow'].agg('sum') / weather_clean.groupby(['Location'])['RainTomorrow'].agg('count'),2))

#Imputamos

weather_clean = weather_clean.replace({
    'WindGustDir': WindGustDir_mean,
    'WindDir9am': WindDir9am_mean,
    'WindDir3pm': WindDir3pm_mean,
    'Location': Location_mean
})

"""

Combinamos las variables numéricas que son medidas a las 9am y 3pm para obtener una única variable por cada condición metereológica. Para ellos creamos una variable para cada par de variables (por ejemplo, Humidity para Humidity9am y Humidity3pm) que tendrá de valor la media de ambas si ninguna es nula, y si una de las dos es nula, contendrá el valor de la otra. Si ambas son nulas el valor quedará nulo."""

# Función para calcular la media o elegir un valor no nulo
def combine_variables(val1, val2):
    if pd.notnull(val1) and pd.notnull(val2):
        return (val1 + val2) / 2
    elif pd.notnull(val1):
        return val1
    elif pd.notnull(val2):
        return val2
    else:
        return np.nan

# Lista de pares de columnas a combinar
columns_to_combine = [('Humidity9am', 'Humidity3pm'), ('Temp9am', 'Temp3pm'), ('WindSpeed9am', 'WindSpeed3pm'), ('Pressure9am', 'Pressure3pm'), ('Cloud9am', 'Cloud3pm'), ('WindDir9am','WindDir3pm')]

# Aplicamos la función
for col1, col2 in columns_to_combine:
    common_name = col1.split('9am')[0].split('3pm')[0].strip()  # obtenemos el nombre de la columna quitándolela parte de la hora
    weather_clean[common_name] = weather_clean.apply(lambda row: combine_variables(row[col1], row[col2]), axis=1)

"""Mediante la combinación de estas variables buscamos simplificar y optimizar el conjunto de datos para el análisis meteorológico. Al realizar esta integración, hemos creado nuevas variables que representan condiciones meteorológicas clave de una manera más concisa y que reduce la complejidad del conjunto de datos original. Además también es beneficioso porque evitamos la pérdida de observaciones debido a valores nulos. Comprobamos cómo han quedado las columnas después de estos cambios:"""

print(weather_clean.head())

"""Las nuevas variables unificadas se han creado correctamente, así que vamos a deshacernos de las originales."""

# Eliminamos las columnas originales
weather_clean.drop(columns=['Humidity9am', 'Humidity3pm', 'Temp9am', 'Temp3pm', 'WindSpeed9am', 'WindSpeed3pm',
                            'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'WindDir9am', 'WindDir3pm'], inplace=True)

"""Echamos un vistazo a la estructura del dataframe una vez hechas estas modificaciones."""

print(weather_clean.info())

"""Ahora tenemos 18 columnas.

## 3. Limpieza de los datos

Vamos a proceder a hacer una limpieza de los datos.

### Ceros


En primer lugar, comprobamos si hay ceros en los datos.
"""

contain_zeros = (weather_clean == 0).any()
print(contain_zeros)

"""Vemos que los hay. Sin embargo, teniendo en cuenta el conocimiento que tenemos sobre los datos, sabemos que los ceros en estas columnas no necesariamente indican un problema, ya que tiene sentido que las variables relacionadas con el tiempo como la temperatura, evaporación, viento, etc puedan contener dicho valor. La temperatura puede ser 0, el viento puede ser 0 km/h, las precipitaciones pueden ser de 0 milímetros (y será el caso en muchas ocasiones ya que la mayoría de días no llueve, tal y como hemos podido comprobar anteriormente al observar las proporciones de las variables). La presencia de ceros, concluimos, es razonable, y no debemos hacer modificaciones al respecto.

### Tratamiento de valores nulos

A continuación exploraremos los valores nulos en nuestro conjunto de datos. En primer lugar, vamos a hacer un recuento de los nulos en cada columna
"""

# Contamos la cantidad de elementos nulos en cada columna
null_count = weather_clean.isnull().sum()
print(null_count)

"""Además, queremos visualizar las proporciones de los valores nulos en cada columna."""

# Calculamos la proporción de elementos nulos en cada columna
null_percentage = (weather_clean.isnull().sum() / len(weather_clean)) * 100

# Lo visualizamos en un gráfico
sns.set(style="whitegrid")

plt.figure(figsize=(12, 6))
null_percentage.plot(kind='bar', color='skyblue')

plt.xlabel('Columnas')
plt.ylabel('Porcentaje de Nulos')
plt.title('Proporción de Elementos Nulos por Columna')

plt.show()

"""Tal y como se observa en el gráfico, hay variables con una alta proporción de valores nulos, por ejemplo Evaporation, Sunshine y Cloud. Habrá que imputarlos para poder trabajar con los datos. Para hacer las imputaciones, diferenciamos entre las variables categóricas.

### Imputación de valores nulos en variables categóricas

Con las variables WindGustDir y RainToday, optamos por remplazar los valores nulos con el valor más frecuente.
"""

# Para las variables categóricas con nulos, podemos remplazar por el valor más frecuente

categoricals_with_nulls = ['WindGustDir', 'RainToday']

for column in categoricals_with_nulls:
    mode_value = weather_clean[column].mode()[0]
    weather_clean[column].fillna(mode_value, inplace=True)

"""### Valores nulos y extremos en variables continuas

Para tratar los valores nulos y hacer la limpieza de las variables numéricas, en primer lugar examinamos los valores extremos. Para ello vamos a visualizar los boxplot de cada variable, así como los histogramas con su distribución.
"""

# Seleccionamos las columnas numéricas (excluyendo RainTomorrow, que es binaria)
numeric_columns = weather_clean.select_dtypes(include=['float64']).columns.drop('RainTomorrow', errors='ignore')


# Hacemos un gráfico con los boxplots
sns.set(style="whitegrid")

num_rows = (len(numeric_columns) + 3) // 4
num_cols = min(4, len(numeric_columns))

fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(16, 4 * num_rows))
axes = axes.flatten()

axes = axes.flatten()

# boxplots individuales para cada variable
for i, column in enumerate(numeric_columns):
    sns.boxplot(x=weather_clean[column], ax=axes[i])
    axes[i].set_title(column)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import matplotlib as mpl

def plot_histograms(data, columns, bins=50, dpi=200, figsize=(30,20)):
    mpl.style.use('default')
    fig = plt.figure(dpi=dpi, figsize=figsize)

    for i, column in enumerate(columns):
        ax = fig.add_subplot(4, 3, i+1)
        ax.hist(data[column], bins=bins)
        ax.set_yscale('log')
        ax.set_title(column)

    plt.show()

columns = ['MinTemp', 'MaxTemp', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'Humidity', 'Temp', 'WindSpeed', 'Pressure', 'Cloud', 'Rainfall']
plot_histograms(weather_clean, columns)

"""En los histogramas podemos observar mejor como las distribuciones de MinTemp y MaxTemp siguen, aparentemente, una distribución normal, además de poder observar los valores extremos aparentes de los diagramas de cajas anteriores. Por ejemplo, en las variables WindSpeed y Evaporation sí podemos ver claramente outliers, mientras que en las demás variables no se observan tan a la ligera. Más adelante probaremos a analizarlos a fondo con otros algoritmos.

Procedemos, por tanto, a eliminar las outliers de las dos variables mencionadas, ya que al ser tan pocos respecto al tamaño total de la muestra, no los consideramos necesarios.
"""

#Eliminamos los registros outliers

weather_clean = weather_clean[weather_clean.Evaporation < 125]
weather_clean = weather_clean[weather_clean.Rainfall < 300]
weather_clean = weather_clean[weather_clean.WindSpeed < 100]

"""Ahora procedemos a tratar los valores nulos. Para ello mostraremos el recuento y los porcentajes de nulos de cada columna."""

#Porcentaje de nulos
counts_null = weather_clean.isnull().sum()
counts_totals = weather_clean.count()
porcentaje_nulos = round(counts_null*100/counts_totals,2).sort_values(ascending=False)
total_nulos = counts_null.sort_values(ascending=False)
print("El número de nulos es")
print(total_nulos)
print("")
print("El porcentaje de nulos es")
print(porcentaje_nulos)

"""Como podemos observar, hay 9 variables con valores nulos. Realmente hay menos con un porcentaje significativo, pero procederemos a completarlos para todos.

Para realizar la imputación de los valores nulos, vamos a probar inicialmente con KNN. Este algoritmo va a buscar los vecinos más cercanos e imputar, mediante KNNImputer, los nulos de cada variable definidia en la lista null_columns, con los valores correspondientes a sus vecinos. Antes de proceder, vamos a probar si realmente podemos aplicar esta imputación. Probaremos diferentes valores de vecinos más cercanos para imputar los valores faltantes y después los compararemos. En este caso, probamos los siguientes valores: 1, 2, 3, 4, 5, 20 y 30.
"""

#Listado de vecinos a testear
n_neighbors = [1, 2, 3, 4, 5, 20, 30]

# Lista de columnas numéricas
null_columns = ['Sunshine', 'Cloud', 'WindGustSpeed', 'Humidity', 'WindDir', 'MaxTemp', 'MinTemp', 'Pressure', 'Temp']

knn_dict = {}
for k in n_neighbors:
    knn_imp = KNNImputer(n_neighbors=k)
    density = knn_imp.fit_transform(weather_clean[null_columns])
    knn_dict[str(k)+'_density'] = pd.DataFrame(density, columns = null_columns)

"""Una vez testeados los distintos valores de K, comparamos los resultados de cada uno visualizándolos en unos gráficos."""

#Ditribución inicial + imputada
bins = 20
for null_column in null_columns:
  fig = plt.figure(figsize=(16, 8))
  counts, bins = np.histogram(weather_clean[null_column].dropna(), bins=20)
  N_nulls = sum(weather_clean[null_column].isna())/(len(bins)-1)
  plt.plot(bins[:-1] + (np.diff(bins)/2)[0], counts, label="Distribución inicial")
  for k in n_neighbors:
    counts, bins = np.histogram(knn_dict[str(k)+'_density'][null_column], bins=20)
    plt.plot(bins[:-1] + (np.diff(bins)/2)[0], counts-N_nulls, label=f"Imputado con k={k}")

  plt.title(null_column)
  plt.legend()
  plt.show()

"""Podemos ver en los gráficos que la línea azul, correspondiente a la distribución inicial, difiere bastante de las imputaciones, las cuales se comportan de una manera basante similar. Por ello, procedemos a elegir el K que, aún así, se asemeja más a la distribución inicial, el k = 2. Vamos a realizar la imputación con KNNImputer.

Cabe destacas que en variables como MinTemp, MaxTemp y Pressure podemos ver que los valores imputados para los distintos K son prácticamente idénticos a la densidad inicial. Esto es debido a que en esas variables la cantidad de null es muy pequeña comparado al set total.
"""

# Imputamos con KNN
imputer = KNNImputer(n_neighbors=2)

weather_clean[null_columns] = imputer.fit_transform(weather_clean[null_columns])

"""Realizamos el recuento de los valores nulos después de realizar KNN para ver si se han imputado correctamente."""

# Comprobamos que los valores nulos se han imputado correctamente

null_count = weather_clean.isnull().sum()
print(null_count)

"""También visualizaremos los boxplot y los histogramas para observar la distribución después de la imputación."""

# Seleccionamos las columnas numéricas (excluyendo RainTomorrow)
numeric_columns = weather_clean.select_dtypes(include=['float64']).columns.drop('RainTomorrow', errors='ignore')


# Hacemos un gráfico con los boxplots
sns.set(style="whitegrid")

num_rows = (len(numeric_columns) + 3) // 4
num_cols = min(4, len(numeric_columns))

fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(16, 4 * num_rows))
axes = axes.flatten()

axes = axes.flatten()

# boxplots individuales para cada variable
for i, column in enumerate(numeric_columns):
    sns.boxplot(x=weather_clean[column], ax=axes[i])
    axes[i].set_title(column)

plt.tight_layout()
plt.show()

"""Dibujamos los histogramas otra vez para ver como queda finalmente el set"""

plot_histograms(weather_clean, columns)

"""## Codificacion de las variables categóricas

Convertimos las variables categóricas a variables dummy.
"""

# Convertir 'No' a 0 y 'Yes' a 1 en RainToday
weather_clean['RainToday'] = weather_clean['RainToday'].map({'No': 0, 'Yes': 1})

"""Finalmente, visualizamos el dataset limpio."""

print(weather_clean.head())
print(weather_clean.info())

"""## 4. Análisis de los datos

### Correlaciones

Para las correlaciones vamos a hacer uso del HeatMap o mapa de calor, el cual mostrará en colores mas rojos o anarjados las correlaciones mas profundas y en azul las no correlaciones.
"""

# Excluimos la columna rowID ya que no aporta información relevante
weather_clean.drop(columns=['row ID'], inplace=True)

#hacemos un heatmap de correlaciones
plt.figure(figsize=(10, 10))
sns.heatmap(weather_clean.corr(), linewidths=0.5, annot=False, fmt=".2f", cmap='coolwarm')
plt.show()

"""En nuestro caso las variables más correlacionadas son:


*   MinTemp con MaxTemp.
*   MaxTemp con Evaporation.
*   Humidity con Cloud

Hemos obviado las correlaciones lógicas como Temp con Min y Max Temp o WindDir con WindGustDir. Esto es interesante saber ya que más adelante utilizaremos algoritmos de regresión y debemos saber si las variables que usaremos para predecir la variable objetivo RainTomorrow tienen correlaciones entre ellas.

### Normalidad de las variables

Para verificar que las variables del dataset siguen una distribución normal podemos elegir entre:

*   Gráficos, como son los histogramas y las gráficas Q-Q.
*   Métodos analíticos, como son los contrastes de hipótesis de Shapiro-Wilk,  D'Agostino K-squared y Kolmogorov Smirnov.

Cabe destacar que el test de Shapiro-Wilk no es muy recomendable para muestras de tamaño >50, debido al Teorema Central del Limite, por lo que, al ser nuestro caso, no lo vamos a utilizar.

El gráfico de histogramas ya lo hemos visto en apartados anteriores, por lo que procederemos a dibujar la gráfica Q-Q.
"""

import statsmodels.api as sm

graph_columns = [('MinTemp',1), ('MaxTemp',2), ('Evaporation',3), ('Sunshine',4), ('WindGustSpeed',5), ('Humidity', 6), ('Temp', 7), ('WindSpeed',8), ('Pressure',9), ('Cloud',10)]

def QQ_graph(graph_columns):

  for column in graph_columns:

    fig = plt.figure(dpi=200, figsize=(30,20))

    fig, ax = plt.subplots(figsize=(7,4))
    sm.qqplot(
        weather_clean[column[0]],
        fit   = True,
        line  = 'q',
        alpha = 0.4,
        lw    = 2,
        ax    = ax
    )

    ax.set_title(column[0], fontsize = 10, fontweight = "bold")

QQ_graph(graph_columns)

"""Aparentemente, algunas de las variables, como ya mencionamos arriba, MinTemp, MaxTemp, Humidity o Pressure, parecen seguir una distribución normal. Por ello, verificamos con D'Agostino's K-squared test si esto es asi tomando un umbral del 0.05. Para ello definimos un contraste de hipótesis tal que H0 es la hipótesis nula en la que asumimos la normalidad de la variable.


*   p-valor <= alpha, por lo que rechazamos la hipótesis nula y asumimos la no normalidad de la variable
*   p-valor > alpha, por lo que aceptamos la hipótesis nula y asumimos la normalidad de la variable.


"""

from scipy import stats

alpha = 0.05
for column in graph_columns:

  k2, p_value = stats.normaltest(weather_clean[column[0]])
  asumpcion = ("podemos asumir que no sigue una distribución normal." if p_value <= alpha  else "podemos asumir que sigue una distribución normal." )

  print(f"Para la variable {column[0]}, con el estadístico {k2} y un p-valor {p_value}, {asumpcion}")

"""Por lo que podemos ver, parece que ninguna de las variables sigue una distribución normal, algo que no esperábamos viendo los gráficos. Comprobamos también con el test de Kolmogorov-Smirnov."""

alpha = 0.05
for column in graph_columns:

  k2, p_value = stats.kstest(weather_clean[column[0]], 'norm')
  asumpcion = ("podemos asumir que no sigue una distribución normal." if p_value <= alpha  else "podemos asumir que sigue una distribución normal." )

  print(f"Para la variable {column[0]}, con el estadístico {k2} y un p-valor {p_value}, {asumpcion}")

"""Observamos, una vez más, que las variables no siguen una distribución normal.

### Homogeneidad de la varianza en las variables no normales

Como hemos visto arriba, ninguna de las variables sigue una distribución normal, por lo que usaremos el test de Levene con la mediana como estadístico de centralidad. Por tanto quedan como hipótesis nula e hipótesis alternativa:


*   H0: los datos proceden de distribuciones con la misma varianza (homocedasticidad).
*   H1: los datos no proceden de distribuciones con la misma varianza, es decir, no se cumple la homocedasticidad.

Por lo tanto, si el p-valor es menor que un determinado valor, el cual tomaremos como 0.05, entonces consideraremos que hay evidencias suficientes para rechazar la homocedasticidad en favor de la heterocedasticidad.
"""

# Test de Levene

alpha = 0.05
for variable in graph_columns:
  levene_test = stats.levene(weather_clean.loc[weather_clean.RainTomorrow == 0, variable[0]], weather_clean.loc[weather_clean.RainTomorrow == 1, variable[0]], center='median')
  homogeneidad = ("podemos asumir la homogeneidad" if levene_test[1] > alpha else "tenemos evidencias suficientes para rechazar la homogeneidad")
  print(f"Para la variable {variable[0]} {homogeneidad}")

"""Como podemos observar solo dos variables cumplirían la homogeneidad: La temperatura mínima y la temperatura media entre las 9am y las 3pm.
Probemos ahora con el test de Fligner-Killeen, ya que este se puede llegar a comportar de mejor forma ante variables de distribución no normal.
"""

# Test de Fligner-Killeen

alpha = 0.05
for variable in graph_columns:
  fligner_test = stats.fligner(weather_clean.loc[weather_clean.RainTomorrow == 0, variable[0]], weather_clean.loc[weather_clean.RainTomorrow == 1, variable[0]], center='median')
  homogeneidad = ("podemos asumir la homogeneidad" if fligner_test[1] > alpha else "tenemos evidencias suficientes para rechazar la homogeneidad")
  print(f"Para la variable {variable[0]} {homogeneidad}")

"""Obtenemos el mismo resultado que con el test de Levene.

## Regresión logística

Debido a que con nuestros datos, vamos a pretender verificar que variables influyen en el hecho de que vaya a llover al dia siguiente o no, es decir, variables categóricas 0 y 1, vamos a aplicar una regresión logística. Importamos, para ello, la librería sklearn.

A continuación, definiremos los conjuntos train y test tomando como variables las que comprobamos arriba que no tenían correlación entre ellas.
"""

pip install plot_confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.weightstats import ttest_ind

# Dividimos de los datos en train y test

X = weather_clean[['MaxTemp', 'MinTemp', 'Rainfall', 'WindGustDir', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'Humidity', 'Pressure', 'Cloud']]
y = weather_clean[['RainTomorrow']]

X_train, X_test, y_train, y_test = train_test_split(
                                        X.values,
                                        y.values,
                                        random_state = 1,
                                    )

# Creamos del modelo
modelo = LogisticRegression(max_iter=300)

#Ajustamos el modelo
modelo.fit(X = X_train, y = y_train.ravel())

# Información del modelo

print("Intercept:", modelo.intercept_)
print("Coeficiente:", list(zip(X.columns, modelo.coef_.flatten(), )))
print("Accuracy de entrenamiento:", modelo.score(X_train , y_train))
print("Accuracy de test:", modelo.score(X_test , y_test))

"""Vemos que la accuracy del test es bastante elevada, de casi un 85%. A esto debemos añadirle que el tamaño del set era bastante elevado

Ahora ya con el modelo entrenado podemos hacer predicciones. En la tabla de abajo observamos la probabilidad de que no llueva al día siguiente con el valor 0 y la probabilidad de que sí llueva al día siguiente con el valor 1.
"""

# Predicciones de la probabilidad

predicciones_probab = modelo.predict_proba(X_test)
predicciones_probab = pd.DataFrame(predicciones_probab, columns = modelo.classes_)
predicciones_probab.head()

"""Ahora podemos predecir, para una observación dada, la clasificación proporcionada por el modelo."""

#Predecimos

predicciones = modelo.predict(X = X_test)

"""Dibujemos una matriz de confusión para entender mejor estas predicciones"""

CM = confusion_matrix(y_test, predicciones, labels=modelo.classes_)
CMD = ConfusionMatrixDisplay(confusion_matrix=CM, display_labels=modelo.classes_)
CMD = CMD.plot(include_values=True, ax=None, xticks_rotation='horizontal', cmap = 'Blues')
plt.grid(False)

"""Como podemos observar que se han predecido correctamente 1.501 lluvias y 10.000 no lluvias, frente a 1.555 y 627 que se han predecido mal, respectivamente.

A continuación probemos con otro modelo de regresión logística proporcionado por Statsmodels.
"""

# Dividimos de los datos en train y test

X = weather_clean[['MaxTemp', 'MinTemp', 'Rainfall', 'WindGustDir', 'MaxTemp', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'Humidity', 'Pressure', 'Cloud']]
y = weather_clean[['RainTomorrow']]

X_train, X_test, y_train, y_test = train_test_split(
                                        X.values,
                                        y.values,
                                        random_state = 2,
                                    )

#Creamos el modelo añadiendo una fila de 1s a set e entranamiento

X_train = sm.add_constant(X_train, prepend=True)
model = sm.Logit(endog=y_train, exog=X_train,)
model = model.fit(method_kwargs={'maxiter':350})
print(model.summary())

"""Arriba podemos ver el error que nos devuelve referente a la convergencia. Esto es debido a que Logit suele converger muy rápido, por lo que por defecto el número máximo de iteraciones, como es este caso, es muy bajo (35). Hemos probado a aumentarlo e incluso así, seguimos obteniendo un error referente a la convergencia. Por tanto, podemos deducir que puede ser debido a que, como hemos visto en gráficos anteriores, la mayoría de las variables caen en un % muy alto en un valor de la variable target, es decir, la mayoría de datos son con RainTomorrow = 0, por lo que el modelo no consigue converger. Esto no tiene por qué ser necesariamente malo, ya que debemos tener en cuenta que en un determinado punto del planeta puede llover más o menos a lo largo de un período de tiempo y eso no significa que la recolección de datos esté mal distribuida.

Por tanto, concluimos que este modelo no se ajusta correctamente a nuestros datos.

Por último, vamos a probar con RandomForest.
"""

# Dividimos de los datos en train y test

X = weather_clean[['MaxTemp', 'MinTemp', 'Rainfall', 'WindGustDir', 'MaxTemp', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'Humidity', 'Pressure', 'Cloud']]
y = weather_clean[['RainTomorrow']]

X_train, X_test, y_train, y_test = train_test_split(
                                        X.values,
                                        y.values,
                                        random_state = 3,
                                        test_size = 0.2
                                    )

#Creamos y entrenamos el modelo

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Información del modelo

print("Accuracy de test:", model_rf.score(X_test , y_test))

#Predecimos obervaciones

predicciones_rf = model_rf.predict(X_test)
predicciones_rf

"""## Resolución del problema.

Consideramos que, tanto el modelo de Regresión logística proporcionado por **sklearn** como el modelo del RandomForest funcionan decentemente con nuestros datos, prediciendo con un accuracy de un 85% aprox cada uno. Por otro lado, el modelo de regresión logística de **statsmodels** no parece haber funcionado, por el motivo de la distribución de las varibles respecto a la variable target que hemos mencionado anteriormente.

Por otro lado, además, cabe destacar que hemos optado, en el análisis de la homogeneidad, por los test más óptimos para las variables que no siguen una distribución normal, pudiéndose usar igualmente las transformaciones de Box-Cox para normalizar dichar variables y posteriormente usar otros test indicados especialmente para distribuciones normales.

## Contribución de las integrantes
"""

tabla_contribuciones = PrettyTable()


tabla_contribuciones.field_names = ['Contribuciones', 'Firma']

# Agregar filas a la tabla
tabla_contribuciones.add_row(['Investigación previa', 'Sofia Holod, Ane Segurola'])
tabla_contribuciones.add_row(['Redacción de las respuestas', 'Sofia Holod, Ane Segurola'])
tabla_contribuciones.add_row(['Desarrollo del código', 'Sofia Holod, Ane Segurola'])
tabla_contribuciones.add_row(['Participación en el vídeo', 'Sofia Holod, Ane Segurola'])

# Imprimir la tabla
print(tabla_contribuciones)
